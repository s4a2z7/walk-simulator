# 개발일지 — Step 2 데이터 수집 준비 및 품질 체크 완료

**작성시각**: 2025-01-05(월)

---

## 해결하고자 한 문제

1. **도메인별(병원/약국/검진센터) 리뷰·문의·FAQ 텍스트 데이터 확보 파이프라인 구축**
   - "합법 범위 내" 수집(공식 사이트 FAQ/안내 페이지 중심)
   - robots.txt 준수 및 PII 마스킹 적용
   - 최종 목표: 350건 이상 텍스트 데이터 확보

2. **초기 샘플 데이터 품질 체크 자동화**
   - 결측/중복/길이/PII 탐지/도메인 분포 자동 리포트
   - 전처리 전 "Raw 데이터 품질 가시화"

---

## 오늘까지 해결된 것(완료)

### 1. 수집 소스 URL 시드 템플릿 추가
**파일**: `data/step2_source_urls.csv`

- 도메인별(병원/약국/검진센터) 수집 대상 URL 구조 정의
- 컬럼: `url_id`, `provider_type`, `source_type`, `url`, `allow_collect`, `notes`
- 현재 10개 시드(예시) 포함, 실제 수집 시 확장 가능

### 2. 웹 텍스트 수집 스크립트 추가
**파일**: `scripts/step2_collect_web_text.py`

- **기능**
  - `data/step2_source_urls.csv` 읽어서 `allow_collect=TRUE` URL만 수집
  - `robots.txt` User-Agent 준수
  - `<p>`, `<li>`, `<div>` 태그 중심 텍스트 추출
  - CSV 스키마(`text_id`, `provider_type`, `source_type`, `text`, `source_url`, `fetched_at`) 준수
- **출력**: `data/step2_collected_raw.csv`

### 3. Raw 데이터 병합 스크립트 추가
**파일**: `scripts/step2_merge_raw_text.py`

- **기능**
  - 웹 수집(`step2_collected_raw.csv`) + 기존 샘플(`step2_raw_text_sample.csv`) 병합
  - `text_id` 중복 제거 후 `data/step2_raw_text_merged.csv`로 저장
- **목적**: 다양한 소스(AI Hub 데이터셋, 웹 수집, 수동 샘플)를 단일 파일로 통합

### 4. 품질 체크 스크립트 추가 및 검증 완료 ✅
**파일**: `scripts/step2_quality_check.py`

- **기능**
  - 필수 컬럼 누락/결측 행 탐지
  - 중복(dedup_key 기준) 카운트
  - PII(URL/이메일/전화) 탐지 및 샘플 추출
  - 도메인/소스 유형 분포
  - 텍스트 길이(원본/정규화 후) 통계
- **출력**
  - `out/step2_quality_report.json` (JSON 형식)
  - `out/step2_quality_report.md` (마크다운 리포트)

**실행 결과(샘플 120건 기준)**:
```
- rows: 120
- 필수 컬럼 누락: 0
- 필수 컬럼 결측 행: 0
- 중복(dedup_key): 0
- PII 탐지: URL(0), EMAIL(0), PHONE(0)
- 도메인 분포: HOSPITAL(52), PHARMACY(36), CHECKUP_CENTER(32)
- 소스 유형: review(70), inquiry(50)
- 텍스트 평균 길이: 17.4자(정규화 후)
```

### 5. 의존성 업데이트
**파일**: `requirements.txt`

- 웹 수집에 필요한 라이브러리 추가
  - `requests==2.32.3`
  - `beautifulsoup4==4.12.3`
- 로컬 환경(Python 3.11) 설치 검증 완료

### 6. README 실행 가이드 업데이트
**파일**: `README.md`

- **Step 2 실행 순서 정리**
  1. (선택) 웹 수집: `py -3 .\scripts\step2_collect_web_text.py`
  2. (선택) Raw 병합: `py -3 .\scripts\step2_merge_raw_text.py`
  3. 품질 체크: `py -3 .\scripts\step2_quality_check.py --raw <경로>`
  4. 전처리: `py -3 .\scripts\step2_preprocess_text.py`
  5. EDA: `py -3 .\scripts\step2_eda_text.py`

---

## 아직 해결되지 않은 것(미완료/리스크)

### 1. 실제 웹 수집 미실행(파이프라인만 구축)
- **현황**: `data/step2_source_urls.csv`에 시드 URL 10개만 포함(예시 수준)
- **리스크**: 350건 목표 달성을 위해 **실제 공식 사이트 URL을 추가**해야 함
- **권장 액션**
  - 공공데이터포털/AI Hub에서 텍스트 데이터셋 다운로드(150건 목표)
  - 병원/약국/검진센터 공식 FAQ 페이지 URL을 `step2_source_urls.csv`에 추가(200건 목표)
  - `step2_collect_web_text.py` 실행 → `step2_merge_raw_text.py` 병합 → 품질 체크

### 2. AI Hub 등 오픈 데이터셋 통합 미완료
- **현황**: 웹 수집 파이프라인만 갖춤
- **권장**: AI Hub "의료/상담/QnA" 데이터셋 다운로드 후 CSV 스키마에 맞춰 변환
- **참고 문서**: `Step2_외부데이터_수집소스_추천.md`

### 3. 멀티모달 데이터(이미지/음성) 미포함
- **현황**: 텍스트만 수집
- **장기 확장**: 진료 예약 시 "증상 사진 첨부", "음성 문의" 기능 고려 시 추가 파이프라인 필요

---

## 향후 개발을 위한 컨텍스트(메모)

### 실행 흐름 요약(Step 2)
```
[1] 데이터 수집
   ├─ 웹(FAQ/안내): scripts/step2_collect_web_text.py
   ├─ 오픈 데이터셋: AI Hub/공공데이터포털 다운로드 후 수동 변환
   └─ 병합: scripts/step2_merge_raw_text.py

[2] 품질 체크(Raw)
   └─ scripts/step2_quality_check.py --raw <경로>

[3] 전처리(정규화/PII 마스킹/중복 제거)
   └─ scripts/step2_preprocess_text.py

[4] EDA(시각화/키워드)
   └─ scripts/step2_eda_text.py
```

### 파일 구조(현재)
```
data/
├─ step2_source_urls.csv           (수집 URL 시드, 10개 예시)
├─ step2_raw_text_sample.csv       (샘플 120건, 품질 OK)
├─ step2_collected_raw.csv         (웹 수집 결과, 아직 미생성)
└─ step2_raw_text_merged.csv       (병합 파일, 아직 미생성)

out/
├─ step2_quality_report.json       (품질 체크 리포트, JSON)
├─ step2_quality_report.md         (품질 체크 리포트, 마크다운)
├─ step2_preprocessed.csv          (전처리 완료)
├─ step2_preprocess_summary.json   (전처리 통계)
└─ eda_*.png / eda_*.csv           (EDA 시각화)
```

### 멘토 세션/제출 시 체크 포인트
1. **합법성 증명**: `robots.txt` 준수 로그, 출처 URL 기록(`source_url` 컬럼)
2. **품질 지표**: `step2_quality_report.md` 제출(결측/중복/PII 처리 증명)
3. **데이터 다양성**: 도메인(병원/약국/검진센터) 균형(45%/35%/20% 권장)
4. **개인정보 보호**: PII 마스킹 적용 후 저장(`step2_preprocess_text.py`)

### 권장 다음 단계(우선순위 순)
1. **AI Hub 데이터셋 다운로드(150건 목표)**
   - 키워드: "의료", "상담", "QnA", "헬스케어"
   - 다운로드 후 CSV 변환 → `step2_merge_raw_text.py`로 병합

2. **공식 사이트 FAQ URL 수집(200건 목표)**
   - `step2_source_urls.csv`에 URL 추가(병원 90건, 약국 70건, 검진센터 40건)
   - `step2_collect_web_text.py` 실행

3. **품질 체크 → 전처리 → EDA 순차 실행**
   - 350건 확보 후 전체 파이프라인 재실행
   - 최종 `out/` 산출물 확인

4. **Step 3(챗봇 통합) 준비**
   - 전처리된 텍스트를 Embedding/RAG 파이프라인에 연결
   - OpenAI API 연동 테스트

---

## 체크리스트(오늘 완료)

- [x] 도메인별 수집 URL 시드 템플릿 추가(`data/step2_source_urls.csv`)
- [x] robots.txt 준수 웹 수집 스크립트 추가(`scripts/step2_collect_web_text.py`)
- [x] Raw 데이터 병합 스크립트 추가(`scripts/step2_merge_raw_text.py`)
- [x] 품질 체크 스크립트 추가(`scripts/step2_quality_check.py`)
- [x] 샘플 데이터(120건) 품질 체크 실행 및 리포트 생성
- [x] `requirements.txt`에 웹 수집 의존성 추가(requests, beautifulsoup4)
- [x] README에 Step 2 실행 순서 정리

---

## 산출물(오늘 생성)

### 코드
- `data/step2_source_urls.csv` — 수집 대상 URL 시드(10개 예시)
- `scripts/step2_collect_web_text.py` — 웹 텍스트 수집(robots 준수)
- `scripts/step2_merge_raw_text.py` — Raw 데이터 병합
- `scripts/step2_quality_check.py` — 품질 체크(결측/중복/PII/분포)

### 리포트
- `out/step2_quality_report.json` — 품질 지표(JSON)
- `out/step2_quality_report.md` — 품질 리포트(마크다운)

### 문서
- `README.md` 업데이트(Step 2 실행 순서)
- `requirements.txt` 업데이트(웹 수집 의존성)

---

**다음 작업**: 실제 공식 사이트 URL 수집(또는 AI Hub 데이터셋 다운로드) → 350건 목표 달성 → 품질 체크 → 전처리 → EDA

