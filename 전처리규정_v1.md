# 전처리 규정 v1 (확정) — 텍스트 데이터(350건 목표)

## 0. 목적
- 공공/오픈 데이터 + 웹 스크래핑으로 수집한 텍스트를 **동일 규칙으로 정제**해 EDA/모델링에 사용 가능하게 만든다.
- 개인정보/추적정보(전화/이메일/URL 등)를 **마스킹**해 저장 리스크를 낮춘다.

---

## 1. 입력/출력
- 입력(예): `data/step2_raw_text_sample.csv`
- 출력:
  - `out/step2_preprocessed.csv`
  - `out/step2_preprocess_summary.json`

---

## 2. 필수 컬럼(없으면 실패)
- `text_id`
- `source_type`
- `provider_type`
- `provider_name`
- `text`

> `source_url`, `created_at/fetched_at`는 강력 권장(특히 스크래핑 데이터는 `source_url` 필수).

---

## 3. 전처리 규칙(확정)

### 3.1 결측치 제거
- 위 “필수 컬럼” 중 1개라도 비어 있으면 제거

### 3.2 텍스트 정규화(`text_normalized`)
- 영문은 소문자 변환
- 허용 문자: 한글/영문/숫자/공백
- 나머지 문자는 공백으로 치환
- 다중 공백은 단일 공백으로 축소

### 3.3 개인정보/추적정보 마스킹
- URL: `<URL>`
- 이메일: `<EMAIL>`
- 전화번호(숫자열): `<PHONE>`
- (선택) 주민등록번호 패턴 등은 `<ID>`로 마스킹(규칙 추가 가능)

### 3.4 길이 필터
- 정규화 후 텍스트 길이 기준 `MIN_TEXT_LEN = 8` 미만 제거

### 3.5 중복 제거
- `dedup_key = provider_type + source_type + text_normalized`
- 동일 `dedup_key`는 1건만 유지

### 3.6 토큰화
- 기본: 공백 토큰화
- 결과 컬럼:
  - `tokens` (리스트를 문자열로 저장 가능)
  - `token_count`

---

## 4. 로그/요약 통계(필수)
`out/step2_preprocess_summary.json`에 아래를 기록한다.
- before: `rows`, `null_text`, `dup_text`
- after: `rows`, `min_len`, `max_len`, `avg_token_count`


